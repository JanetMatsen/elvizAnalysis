{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pd.options.display.max_rows = 999\n",
    "#pd.get_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Control whether to import the original data. \n",
    "import_original_data = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mila:\n",
    "\n",
    "OK only four files needed to be corrected. I put them in your Meta4 folder under the name Elviz_Corrected\n",
    "\n",
    "The remaining four files indeed contained fewer contigs.\n",
    "\n",
    "Thanks!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python\n",
      "2.7.5 (default, Mar  9 2014, 22:15:05) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 5.0 (clang-500.0.68)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print sys.executable \n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>oxy</th>\n",
       "      <th>rep</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th>project</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_LOW4</th>\n",
       "      <th>1056013</th>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13_LOW5</th>\n",
       "      <th>1056037</th>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25_LOW6</th>\n",
       "      <th>1056061</th>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37_LOW7</th>\n",
       "      <th>1056085</th>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49_LOW8</th>\n",
       "      <th>1056109</th>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 oxy  rep  week\n",
       "ID      project                \n",
       "1_LOW4  1056013  Low    1     4\n",
       "13_LOW5 1056037  Low    1     5\n",
       "25_LOW6 1056061  Low    1     6\n",
       "37_LOW7 1056085  Low    1     7\n",
       "49_LOW8 1056109  Low    1     8"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the csv that translates the 127_HOW14 type labels to weeks and replicates.\n",
    "\n",
    "from elviz_utils import IMPORT_METAINFO_TYPES, read_sample_info\n",
    "\n",
    "\n",
    "sample_info = read_sample_info()\n",
    "\n",
    "# set index for merging with the Elviz data. \n",
    "sample_info.set_index(keys=['ID', 'project'], inplace=True)\n",
    "\n",
    "# drop boring columns\n",
    "sample_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elviz_utils import IMPORT_DATA_TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "# loop over each file, collapse rows that share phylogeny data to the genus label,\n",
    "# save new .csv files, then append them all together in a separate step. \n",
    "#filepath = '/Volumes/Shares/Users/Janet/meta4/160121_elviz_files_updated/'\n",
    "filepath = './data'\n",
    "elviz_files = [f for f in os.listdir(filepath) if \".csv\" in f]\n",
    "print len(elviz_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reduce_elviz_to_genus_rpk(df):\n",
    "    '''\n",
    "    Take in a raw elviz DataFrame and return a Pandas array with a sum at the genus level'''\n",
    "    # get rid of undesired columns because the data is big.\n",
    "    columns_to_drop = ['datasetId', 'contigId', \n",
    "                       'Average fold', \n",
    "                       'Reference GC','Covered percent','Covered bases',\n",
    "                       #'Plus reads', 'Minus reads', \n",
    "                       'Median fold',\n",
    "                       'Read GC','Complete Lineage', 'MG scaffold_oid']\n",
    "    for c in columns_to_drop:\n",
    "        if c in df.columns:\n",
    "            del df[c]\n",
    "    # rename NaN at Genus to \"other\"  \n",
    "    df.Genus.replace(np.nan, \"other\", inplace=True)\n",
    "    \n",
    "    # calculat RPK: sum the reads that mapped to the plus strand and minus strand\n",
    "    df['reads per kilobase'] = (df['Plus reads'] + df['Minus reads'])/(df['Length']/1000.)\n",
    "\n",
    "    ## calculate coverage * length.\n",
    "    #df['fold_times_length'] = df['Average fold']*df['Length']\n",
    "\n",
    "    # now we can collapse by shared genus\n",
    "    #df = df.groupby(['Kingdom','Phylum','Class','Order','Family','Genus'])['fold_times_length'].sum().reset_index()\n",
    "    df = df.groupby(['Kingdom','Phylum','Class','Order','Family','Genus'])['reads per kilobase'].sum().reset_index()\n",
    "    # rename our new measure of abundance: \n",
    "    df.rename(columns={'reads per kilobase': 'sum of reads per kilobase'}, inplace=True)\n",
    "    #df.rename(columns={'fold_times_length': 'sum of fold_times_length'}, inplace=True)\n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elviz_1056013_1_LOW4.csv:  1_LOW4\n",
      "elviz_1056079_33_HOW6.csv:  33_HOW6\n",
      "elviz_1056244_116_HOW13.csv:  116_HOW13\n"
     ]
    }
   ],
   "source": [
    "def id_from_filename(s):\n",
    "    return re.search('[\\w]+_[0-9]+_([0-9]+_[HL]OW[0-9]+).csv', s).group(1)\n",
    "\n",
    "# test\n",
    "print elviz_files[0]+\": \", id_from_filename(elviz_files[0])\n",
    "print elviz_files[22]+\": \", id_from_filename(elviz_files[22])\n",
    "print elviz_files[77]+\": \", id_from_filename(elviz_files[77])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elviz_1056013_1_LOW4.csv:  1056013\n",
      "elviz_1056079_33_HOW6.csv:  1056079\n",
      "elviz_1056244_116_HOW13.csv:  1056244\n"
     ]
    }
   ],
   "source": [
    "def project_number_from_filename(s):\n",
    "    return re.search('[\\w]+_([0-9]+)_[0-9]+_[HL]OW[0-9]+.csv', s).group(1)\n",
    "\n",
    "# test\n",
    "print elviz_files[0]+\": \", project_number_from_filename(elviz_files[0])\n",
    "print elviz_files[22]+\": \", project_number_from_filename(elviz_files[22])\n",
    "print elviz_files[77]+\": \", project_number_from_filename(elviz_files[77])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate abundance.  Note that apply runs the 1st loop item twice:\n",
    "    # http://stackoverflow.com/questions/21635915/why-does-pandas-apply-calculate-twice\n",
    "def norm_by_ID(group):\n",
    "    #print group\n",
    "    #fold = group['sum of fold_times_length']\n",
    "    fold = group['sum of reads per kilobase']\n",
    "    #print rpk.head()\n",
    "    #print sum(rpk)\n",
    "    #print \"\"\n",
    "    group['sum of reads per kilobase'] = fold/sum(fold)\n",
    "    return group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_and_reduce_elviz_csv(filename, filepath):\n",
    "    df = pd.read_csv(filepath + '/' + filename, sep=\",\", dtype=IMPORT_DATA_TYPES, usecols=range(1,20))\n",
    "    # reduce to one row per genus with reduce_elviz_to_genus_rpk()\n",
    "    # Need to use DataFrame to convert the seris back to a dataframe if you want to add an ID column.\n",
    "    df = pd.DataFrame(reduce_elviz_to_genus_rpk(df))\n",
    "    # add an ID label.  (don't use 'id'; that is a Python built-in.)\n",
    "    df['ID'] = id_from_filename(filename)\n",
    "    df['project'] = project_number_from_filename(filename)\n",
    "    \n",
    "    # merge on the sample_info. \n",
    "    print df.head()\n",
    "    print sample_info.head()\n",
    "    df = df.set_index(keys=['ID', 'project'], inplace=True)\n",
    "    df = pd.merge(df, sample_info)\n",
    "    #df = pd.merge(df.reset_index(), sample_info, on='ID') # ['ID', 'project']\n",
    "    #df = df.reset_index().join(other = sample_info, on=['ID', 'project'], how='left') # ['ID', 'project']\n",
    "    print df.head()\n",
    "    df = df.groupby('ID').apply(norm_by_ID)\n",
    "    # after norm_by_ID is applied, 'Average fold' is now a pooled number.\n",
    "    # rename column to abundance since we normalized it. \n",
    "    df.rename(columns={'sum of reads per kilobase':'abundance'}, inplace=True)\n",
    "    # sort so most abundant is on top. \n",
    "    print df.head()\n",
    "    df.sort_values(by='abundance', axis=0, ascending=False, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['elviz_1056016_2_LOW4.csv', 'elviz_1056019_3_LOW4.csv', 'elviz_1056022_4_LOW4.csv']\n"
     ]
    }
   ],
   "source": [
    "# import the raw or raw-ish data. \n",
    "if import_original_data:\n",
    "    filepath = '/Users/janet/Documents/Lidstrom Lab Work/meta4/160125_Elviz_files_corrected'\n",
    "    elviz_files = [f for f in os.listdir(filepath) if \".csv\" in f]\n",
    "    print elviz_files[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./results'):\n",
    "    os.makedirs('./results/')\n",
    "if not os.path.exists('./plots'):\n",
    "    os.makedirs('./plots/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Kingdom         Phylum         Class            Order            Family  \\\n",
      "0  Archaea  Crenarchaeota  Thermoprotei  Thermoproteales  Thermoproteaceae   \n",
      "1  Archaea  Euryarchaeota  Halobacteria  Halobacteriales  Halobacteriaceae   \n",
      "2  Archaea  Euryarchaeota  Halobacteria  Halobacteriales  Halobacteriaceae   \n",
      "3  Archaea  Euryarchaeota  Halobacteria  Halobacteriales  Halobacteriaceae   \n",
      "4  Archaea  Euryarchaeota  Halobacteria  Halobacteriales  Halobacteriaceae   \n",
      "\n",
      "             Genus  sum of reads per kilobase      ID  project  \n",
      "0     Vulcanisaeta                  29.213483  1_LOW4  1056013  \n",
      "1  Halalkalicoccus                  41.415663  1_LOW4  1056013  \n",
      "2    Halobacterium                  87.530232  1_LOW4  1056013  \n",
      "3      Halobiforma                  66.776587  1_LOW4  1056013  \n",
      "4       Halococcus                  40.976460  1_LOW4  1056013  \n",
      "                 oxy  rep  week\n",
      "ID      project                \n",
      "1_LOW4  1056013  Low    1     4\n",
      "13_LOW5 1056037  Low    1     5\n",
      "25_LOW6 1056061  Low    1     6\n",
      "37_LOW7 1056085  Low    1     7\n",
      "49_LOW8 1056109  Low    1     8\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-853d7d6604d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_reduce_elviz_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melviz_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-89-25fa0ec55f0f>\u001b[0m in \u001b[0;36mread_and_reduce_elviz_csv\u001b[0;34m(filename, filepath)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0msample_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'project'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m#df = pd.merge(df.reset_index(), sample_info, on='ID') # ['ID', 'project']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#df = df.reset_index().join(other = sample_info, on=['ID', 'project'], how='left') # ['ID', 'project']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/tools/merge.pyc\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[1;32m     32\u001b[0m                          \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_on\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleft_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                          copy=copy, indicator=indicator)\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/tools/merge.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[1;32m    188\u001b[0m         (self.left_join_keys,\n\u001b[1;32m    189\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m          self.join_names) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/tools/merge.pyc\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mleft_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \"\"\"\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_specification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mleft_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/tools/merge.pyc\u001b[0m in \u001b[0;36m_validate_specification\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0;31m# use the common columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m                 common_cols = self.left.columns.intersection(\n\u001b[0m\u001b[1;32m    467\u001b[0m                     self.right.columns)\n\u001b[1;32m    468\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommon_cols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "tmp = read_and_reduce_elviz_csv(filename = elviz_files[0], filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    project_x  project_y\n",
      "642   1056013    1056013\n",
      "<type 'str'>\n",
      "<type 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "tmp2 = tmp[['project_x','project_y']]\n",
    "print tmp2.drop_duplicates()\n",
    "print type(tmp2.project_x[0])\n",
    "print type(tmp2.project_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     index   Kingdom          Phylum                Class             Order  \\\n",
      "642    642  Bacteria  Proteobacteria  Gammaproteobacteria   Methylococcales   \n",
      "494    494  Bacteria  Proteobacteria   Betaproteobacteria   Methylophilales   \n",
      "198    198  Bacteria   Bacteroidetes       Flavobacteriia  Flavobacteriales   \n",
      "650    650  Bacteria  Proteobacteria  Gammaproteobacteria   Methylococcales   \n",
      "478    478  Bacteria  Proteobacteria   Betaproteobacteria   Burkholderiales   \n",
      "\n",
      "                Family           Genus  abundance      ID  project  oxy  rep  \\\n",
      "642   Methylococcaceae   Methylobacter   0.275835  1_LOW4  1056013  Low    1   \n",
      "494   Methylophilaceae   Methylotenera   0.266572  1_LOW4  1056013  Low    1   \n",
      "198  Flavobacteriaceae  Flavobacterium   0.066042  1_LOW4  1056013  Low    1   \n",
      "650   Methylococcaceae           other   0.057879  1_LOW4  1056013  Low    1   \n",
      "478     Comamonadaceae           other   0.036590  1_LOW4  1056013  Low    1   \n",
      "\n",
      "     week  \n",
      "642     4  \n",
      "494     4  \n",
      "198     4  \n",
      "650     4  \n",
      "478     4  \n",
      "elviz_1056016_2_LOW4.csv\n",
      "elviz_1056019_3_LOW4.csv\n",
      "elviz_1056022_4_LOW4.csv\n",
      "elviz_1056025_7_HOW4.csv\n",
      "elviz_1056028_8_HOW4.csv\n",
      "elviz_1056031_9_HOW4.csv\n",
      "elviz_1056034_10_HOW4.csv\n",
      "elviz_1056037_13_LOW5.csv\n",
      "elviz_1056040_14_LOW5.csv\n",
      "elviz_1056043_15_LOW5.csv\n",
      "elviz_1056046_16_LOW5.csv\n",
      "elviz_1056049_19_HOW5.csv\n",
      "elviz_1056052_20_HOW5.csv\n",
      "elviz_1056055_21_HOW5.csv\n",
      "elviz_1056058_22_HOW5.csv\n",
      "elviz_1056061_25_LOW6.csv\n",
      "elviz_1056064_26_LOW6.csv\n",
      "elviz_1056067_27_LOW6.csv\n",
      "elviz_1056070_28_LOW6.csv\n",
      "elviz_1056073_31_HOW6.csv\n",
      "elviz_1056076_32_HOW6.csv\n",
      "elviz_1056079_33_HOW6.csv\n",
      "elviz_1056082_34_HOW6.csv\n",
      "elviz_1056085_37_LOW7.csv\n",
      "elviz_1056088_38_LOW7.csv\n",
      "elviz_1056091_39_LOW7.csv\n",
      "elviz_1056094_40_LOW7.csv\n",
      "elviz_1056097_43_HOW7.csv\n",
      "elviz_1056100_44_HOW7.csv\n",
      "elviz_1056103_45_HOW7.csv\n",
      "elviz_1056106_46_HOW7.csv\n",
      "elviz_1056109_49_LOW8.csv\n",
      "elviz_1056112_50_LOW8.csv\n",
      "elviz_1056115_51_LOW8.csv\n",
      "elviz_1056118_52_LOW8.csv\n",
      "elviz_1056121_55_HOW8.csv\n",
      "elviz_1056124_56_HOW8.csv\n",
      "elviz_1056127_57_HOW8.csv\n",
      "elviz_1056130_58_HOW8.csv\n",
      "elviz_1056133_61_LOW9.csv\n",
      "elviz_1056136_62_LOW9.csv\n",
      "elviz_1056139_63_LOW9.csv\n",
      "elviz_1056142_64_LOW9.csv\n",
      "elviz_1056145_67_HOW9.csv\n",
      "elviz_1056148_68_HOW9.csv\n",
      "elviz_1056151_69_HOW9.csv\n",
      "elviz_1056154_70_HOW9.csv\n",
      "elviz_1056157_73_LOW10.csv\n",
      "elviz_1056160_74_LOW10.csv\n",
      "elviz_1056163_75_LOW10.csv\n",
      "elviz_1056166_76_LOW10.csv\n",
      "elviz_1056169_79_HOW10.csv\n",
      "elviz_1056172_80_HOW10.csv\n",
      "elviz_1056175_81_HOW10.csv\n",
      "elviz_1056178_82_HOW10.csv\n",
      "elviz_1056181_85_LOW11.csv\n",
      "elviz_1056184_86_LOW11.csv\n",
      "elviz_1056187_87_LOW11.csv\n",
      "elviz_1056190_88_LOW11.csv\n",
      "elviz_1056193_91_HOW11.csv\n",
      "elviz_1056196_92_HOW11.csv\n",
      "elviz_1056199_93_HOW11.csv\n",
      "elviz_1056202_94_HOW11.csv\n",
      "elviz_1056205_97_LOW12.csv\n",
      "elviz_1056208_98_LOW12.csv\n",
      "elviz_1056211_99_LOW12.csv\n",
      "elviz_1056214_100_LOW12.csv\n",
      "elviz_1056217_103_HOW12.csv\n",
      "elviz_1056220_104_HOW12.csv\n",
      "elviz_1056223_105_HOW12.csv\n",
      "elviz_1056226_106_HOW12.csv\n",
      "elviz_1056229_109_LOW13.csv\n",
      "elviz_1056232_110_LOW13.csv\n",
      "elviz_1056235_111_LOW13.csv\n",
      "elviz_1056238_112_LOW13.csv\n",
      "elviz_1056241_115_HOW13.csv\n",
      "elviz_1056244_116_HOW13.csv\n",
      "elviz_1056247_117_HOW13.csv\n",
      "elviz_1056250_118_HOW13.csv\n",
      "elviz_1056253_121_LOW14.csv\n",
      "elviz_1056256_122_LOW14.csv\n",
      "elviz_1056259_123_LOW14.csv\n",
      "elviz_1056262_124_LOW14.csv\n",
      "elviz_1056265_127_HOW14.csv\n",
      "elviz_1056268_128_HOW14.csv\n",
      "elviz_1056271_129_HOW14.csv\n",
      "elviz_1056274_130_HOW14.csv\n",
      "775164\n"
     ]
    }
   ],
   "source": [
    "if import_original_data:\n",
    "    data_reduced = read_and_reduce_elviz_csv(filename = elviz_files[0], filepath=filepath)\n",
    "    for f in elviz_files[1:]:\n",
    "        # read and write a csv for that sample\n",
    "        df_to_add = read_and_reduce_elviz_csv(filename = f, filepath=filepath)\n",
    "        data_reduced = data_reduced.append(df_to_add)\n",
    "        print f  # prints filename\n",
    "        #print data_reduced.head(1)\n",
    "    print data_reduced.size  \n",
    "else: \n",
    "    data_reduced = pd.read_csv(\"./results/reduced_data--all_phylogeny_remains.csv\")\n",
    "\n",
    "data_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract out the project number and ID:\n",
    "id_and_proj = data_reduced[['ID', 'project']].drop_duplicates()\n",
    "sample_info_augmented = sample_info.merge(right=id_and_proj, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>oxy</th>\n",
       "      <th>rep</th>\n",
       "      <th>week</th>\n",
       "      <th>project</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_LOW4</td>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1056013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13_LOW5</td>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1056037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25_LOW6</td>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1056061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37_LOW7</td>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1056085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49_LOW8</td>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1056109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  oxy  rep  week  project\n",
       "0   1_LOW4  Low    1     4  1056013\n",
       "1  13_LOW5  Low    1     5  1056037\n",
       "2  25_LOW6  Low    1     6  1056061\n",
       "3  37_LOW7  Low    1     7  1056085\n",
       "4  49_LOW8  Low    1     8  1056109"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print sample_info_augmented.shape\n",
    "sample_info_augmented.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_info_augmented.sort_values(by='project',)\n",
    "sample_info_augmented.to_csv('data/sample_meta_info.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Should have only 1 row per Id now. \n",
    "data_reduced[data_reduced['Genus']=='Methylophilus']\n",
    "\n",
    "if import_original_data:\n",
    "    data_reduced.to_csv(\"./results/reduced_data--all_phylogeny_remains.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_reduced.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make sure the abundances all sum to 1. \n",
    "for t, d in data_reduced.groupby('ID'):\n",
    "    print t, \":\", d['abundance'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_reduced.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check number of sample imported.  Dave had a problem with the raw files 1/16/2015 using Python 3. \n",
    "len(data_reduced.ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_reduced['sample_name'] = 'replicate '+ data_reduced['rep'].astype(str) +\": \" + data_reduced['oxy'] + ' O2'\n",
    "print data_reduced['sample_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_reduced.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write an excel for Mila.  One notebook for replicate.  Each page is a different ID/week. \n",
    "by_repl_and_week = data_reduced.groupby(['rep','week','oxy'])#.to_excel(writer,sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The groupby returns tuples of the conditions and the data in a tuple. \n",
    "#for (rep, week, oxy), d in by_repl_and_week:\n",
    "#    print \"rep_\" + str(rep)\n",
    "#    print \"week_\" + str(week)\n",
    "#    print oxy\n",
    "#    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_reduced['rep'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write same dictionary in a loop\n",
    "excel_files = {}\n",
    "for ox in data_reduced['oxy'].unique():\n",
    "    for re in data_reduced['rep'].unique():\n",
    "        print ox, re\n",
    "        excel_files[(ox, re)] =  'elviz_binned--{}O2_rep{}.xlsx'.format(ox, re)\n",
    "  \n",
    "print excel_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Turn off re-writ here:\n",
    "\n",
    "write_excel = True\n",
    "if import_original_data and write_excel:\n",
    "#if write_excel:\n",
    "    # make an excel writer for each file. \n",
    "    writer_dict = {}\n",
    "    for oxy in data_reduced['oxy'].unique():\n",
    "        for rep in data_reduced['rep'].unique():\n",
    "            #print ox, re\n",
    "            filename = 'elviz--{}O2_rep_{}.xlsx'.format(oxy, rep)\n",
    "            #print filename\n",
    "            writer_dict[(oxy, rep)] = pd.ExcelWriter(filename, engine='xlsxwriter')\n",
    "            #print writer_dict\n",
    "    print writer_dict\n",
    "    \n",
    "    # The groupby returns tuples of the conditions. \n",
    "    by_repl_and_week = data_reduced.groupby(['rep','week','oxy'])#.to_excel(writer,sheet_name='Sheet1')  \n",
    "    for (rep, week, oxy), d in by_repl_and_week:\n",
    "        # use the writer that matches the replicate:\n",
    "        print rep, week, oxy\n",
    "        writer = writer_dict[(oxy, rep)]\n",
    "        sheet_name = oxy + '_O2' +\"_rep_\" + str(rep)+\"_week_\" + str(week)\n",
    "        print sheet_name\n",
    "        d.reset_index()\n",
    "        #print d.columns\n",
    "        del d['index']\n",
    "        #print d.columns\n",
    "        d.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        #writer.save()\n",
    "        print \"\"\n",
    "    \n",
    "    # close each writer.  This saves them. \n",
    "    for w_dict in writer_dict.values():\n",
    "        w_dict.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_reduced.week.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Another check that we have each ID's abundances summing to 1. \n",
    "for t, d in data_reduced.groupby('ID'):\n",
    "    if d['abundance'].sum() > 1.0001:\n",
    "        print t, \":\", d['abundance'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Further reduce data so only Genus is preserved. \n",
    "# drop the reads per kilobase or we will get more than 1 row for other.\n",
    "data_reduced_genus = data_reduced.copy()\n",
    "#del data_reduced_genus['reads per kilobase']\n",
    "print data_reduced_genus.head()\n",
    "data_reduced_genus = data_reduced_genus.groupby(['ID','rep','week','oxy','Genus']).sum().reset_index()\n",
    "data_reduced_genus.sort_values(by=['rep', 'abundance'], inplace=True, ascending=False)\n",
    "data_reduced_genus.tail(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_reduced_genus.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_reduced_genus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data_reduced_genus[['rep','week', 'ID']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_reduced_genus.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "by_repl_and_week_Genus = data_reduced_genus.groupby(['rep','week','oxy'])#.to_excel(writer,sheet_name='Sheet1')\n",
    "# check that replicates and week are in the right order: \n",
    "#for tup, d in by_repl_and_week_Genus:\n",
    "#    print tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Turn off re-writ here:\n",
    "write_excel = True\n",
    "\n",
    "if write_excel:\n",
    "    # make an excel writer for each file. \n",
    "    writer_dict = {}\n",
    "    for oxy in data_reduced['oxy'].unique():\n",
    "        for rep in data_reduced['rep'].unique():\n",
    "            #print ox, re\n",
    "            filename = 'elviz--Genus_only--{}O2_rep_{}.xlsx'.format(oxy, rep)\n",
    "            #print filename\n",
    "            writer_dict[(oxy, rep)] = pd.ExcelWriter(filename, engine='xlsxwriter')\n",
    "            #print writer_dict\n",
    "    print writer_dict\n",
    "    \n",
    "    # The groupby returns tuples of the conditions. \n",
    "    by_repl_and_week = data_reduced_genus.groupby(['rep','week','oxy'])#.to_excel(writer,sheet_name='Sheet1')  \n",
    "    for (rep, week, oxy), d in by_repl_and_week:\n",
    "        # use the writer that matches the replicate:\n",
    "        print rep, week, oxy\n",
    "        writer = writer_dict[(oxy, rep)]\n",
    "        sheet_name = oxy + '_O2' +\"_rep_\" + str(rep)+\"_week_\" + str(week)\n",
    "        print sheet_name\n",
    "        d.reset_index(inplace=True)\n",
    "        del d['index']\n",
    "        #print d.columns\n",
    "        del d['level_0']\n",
    "        d.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        #writer.save()\n",
    "        print \"\"\n",
    "    \n",
    "    # close each writer.  This saves them. \n",
    "    for w_dict in writer_dict.values():\n",
    "        w_dict.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Demo of simple plot.  Trim rows with abundance some percent.  Plot only one week/oxygen combo. \n",
    "def plot_by_abundance(data, abundance):\n",
    "    d_plot = data[data['abundance'] > abundance]\n",
    "    d_plot = pd.pivot_table(d_plot, values='abundance', index=['week'], columns='Genus') #.reset_index()\n",
    "    print d_plot.head()\n",
    "    d_plot.plot(kind='bar',stacked=True)\n",
    "    sns.despine()\n",
    "    #plt.legend(loc=2)\n",
    "plot_by_abundance(data=data_reduced_genus[\n",
    "        np.logical_and(\n",
    "            data_reduced_genus['rep']==1, \n",
    "            data_reduced_genus['oxy']==\"Low\")], abundance=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Begin to generalize.  Want to be able to subset by oxygen and replicate. \n",
    "def subset_data_by_rep_and_oxy(data, rep, oxy):\n",
    "    return data[\n",
    "        np.logical_and(\n",
    "            data_reduced_genus['rep']==rep, \n",
    "            data_reduced_genus['oxy']==oxy)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make an individual plot with the new subset_data_by_rep_and_oxy() function\n",
    "def plot_by_abundance(data, abundance):\n",
    "    d_plot = data[data['abundance'] > abundance]\n",
    "    d_plot = pd.pivot_table(d_plot, values='abundance', index=['week'], columns='Genus') #.reset_index()\n",
    "    #print d_plot.head()\n",
    "    d_plot.plot(kind='bar',stacked=True)\n",
    "    sns.despine()\n",
    "    #plt.legend(loc=2)\n",
    "plot_by_abundance(subset_data_by_rep_and_oxy(data= data_reduced_genus, rep=1, oxy='High'), abundance=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up a figure with four subplots. \n",
    "fig, axs = plt.subplots(2, 4, figsize=(8, 6))  # ax belongs to fig. \n",
    "print fig\n",
    "print axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = data_reduced_genus.groupby(['rep', 'oxy'])\n",
    "for t, dg in g:\n",
    "    print t #, dg\n",
    "    #print t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row_dict = {(1, 'High') : 2,\n",
    "                (1, 'Low')  : 1,\n",
    "                (2, 'High') : 2,\n",
    "                (2, 'Low')  : 1,\n",
    "                (3, 'High') : 2,\n",
    "                (3, 'Low')  : 1,\n",
    "                (4, 'High') : 2,\n",
    "                (4, 'Low')  : 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print axs\n",
    "print \"\"\n",
    "axs[(1-1)][2-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#axd = {1:axs[0], 2:axs[1], 3:axs[2], 4:axs[3]}\n",
    "title_list = ''\n",
    "\n",
    "for t, df in g:\n",
    "    #print t\n",
    "    #print 'plot row:', row_dict[t]\n",
    "    #print 'plot column:', t[0]\n",
    "    df.plot(kind='scatter', ax=axs[row_dict[t]-1][t[0]-1], x='week', y='abundance', title='meow')\n",
    "\n",
    "sns.despine()  # Removes the boxes around the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up a figure with four subplots. \n",
    "fig, axs = plt.subplots(2, 4, figsize=(12, 10))  # ax belongs to fig. \n",
    "#print fig\n",
    "#print axs\n",
    "\n",
    "# set up groupby \n",
    "\n",
    "d_plot = data_reduced_genus[data_reduced_genus['abundance'] > 0.05]\n",
    "g = d_plot.groupby(['rep', 'oxy'])\n",
    "\n",
    "#axd = {1:axs[0], 2:axs[1], 3:axs[2], 4:axs[3]}\n",
    "title_list = ''\n",
    "\n",
    "row_dict = {(1, 'High') : 2, (2, 'High') : 2, (3, 'High') : 2, (4, 'High') : 2,\n",
    "            (1, 'Low')  : 1, (2, 'Low')  : 1, (3, 'Low')  : 1, (4, 'Low')  : 1}\n",
    "\n",
    "for tup, df in g:\n",
    "    # cast the data\n",
    "    #print tup # tuple\n",
    "    data_pivoted = pd.pivot_table(df, values='abundance', \n",
    "                                        index=['week'],  # drops all the other columns. \n",
    "                                        columns='Genus') \n",
    "    #print data_pivoted.head()\n",
    "    #df.groupby('Kingdom', 'Phylum', 'Class', 'Order', 'Family', 'Genus', 'ID', 'oxy', 'rep')\n",
    "    #print axs[row_dict[tup]-1][tup[0]-1]\n",
    "    data_pivoted.plot(kind='bar', ax=axs[row_dict[tup]-1][tup[0]-1], stacked=True)  # don't specify x b/c there is only 1 index. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fig  # shows the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## We can get the last one out, but we want all of them out and want them to have shared colors. \n",
    "# set up a figure with four subplots. \n",
    "fig, axs = plt.subplots(2, 4, figsize=(12, 10))  # ax belongs to fig. \n",
    "#print fig\n",
    "print axs\n",
    "\n",
    "# set up groupby \n",
    "\n",
    "d_plot = data_reduced_genus[data_reduced_genus['abundance'] > 0.05]\n",
    "g = d_plot.groupby(['rep', 'oxy'])\n",
    "\n",
    "#axd = {1:axs[0], 2:axs[1], 3:axs[2], 4:axs[3]}\n",
    "title_list = ''\n",
    "\n",
    "row_dict = {(1, 'High') : 2, (2, 'High') : 2, (3, 'High') : 2, (4, 'High') : 2,\n",
    "            (1, 'Low')  : 1, (2, 'Low')  : 1, (3, 'Low')  : 1, (4, 'Low')  : 1}\n",
    "\n",
    "for tup, df in g:\n",
    "    # cast the data\n",
    "    #print tup # tuple\n",
    "    data_pivoted = pd.pivot_table(df, values='abundance', \n",
    "                                        index=['week'],  # drops all the other columns. \n",
    "                                        columns='Genus') \n",
    "    #print data_pivoted.head()\n",
    "    #df.groupby('Kingdom', 'Phylum', 'Class', 'Order', 'Family', 'Genus', 'ID', 'oxy', 'rep')\n",
    "    #print axs[row_dict[tup]-1][tup[0]-1]\n",
    "    data_pivoted.plot(kind='bar', ax=axs[row_dict[tup]-1][tup[0]-1], stacked=True)  # don't specify x b/c there is only 1 index. \n",
    "\n",
    "plt.legend(loc = 'lower center', bbox_to_anchor = (0,-0.1,1,1),\n",
    "            bbox_transform = plt.gcf().transFigure )            \n",
    "#legend = plt.legend(loc='upper center', shadow=True, fontsize='x-large')   \n",
    "#fig.legend(bbox_to_anchor=(1.05, 0), loc='lower left', borderaxespad=0.)\n",
    "#         # it will place the legend on the outer right-hand side of the last axes    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "methods=['method 1', 'method2', 'method 3', 'method 4']\n",
    "times = range(0, 100, 10)\n",
    "data = pd.DataFrame(list(itertools.product(methods, times, times)))\n",
    "data.columns = ['method', 'dtsi','rtsi']\n",
    "data['nw_score'] = np.random.sample(data.shape[0])\n",
    "print data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preview of pivot that happens:  (note use of pivot_table, not pivot.  Aggregating across duplicate rows somehow.)\n",
    "data.pivot_table(index=\"dtsi\", columns='rtsi', values='nw_score').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['dtsi'].unique()  # x-values\n",
    "# rtsi are y-values\n",
    "# cast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_reduced_genus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a heat map.  Then we aren't confused by colors, and overwhelmed by the # of labels. \n",
    "d_plot = data_reduced_genus[data_reduced_genus['abundance'] > 0.05]\n",
    "\n",
    "def facet_heatmap(data, color, **kws):\n",
    "    data = data.pivot(index='Genus', columns='week', values='abundance')\n",
    "    sns.heatmap(data, cmap='Blues', **kws)  # <-- Pass kwargs to heatmap\n",
    "\n",
    "with sns.plotting_context(font_scale=5.5):\n",
    "    g = sns.FacetGrid(d_plot[d_plot['oxy']=='Low'], \n",
    "                      col=\"rep\", size=3, aspect=1)\n",
    "\n",
    "cbar_ax = g.fig.add_axes([.92, .3, .02, .4])  # <-- Create a colorbar axes\n",
    "\n",
    "g = g.map_dataframe(facet_heatmap,\n",
    "                    cbar_ax=cbar_ax,\n",
    "                    vmin=0, vmax=1)  # <-- Specify the colorbar axes and limits\n",
    "\n",
    "g.set_titles(col_template=\"{col_name}\", fontweight='bold', fontsize=18)\n",
    "g.fig.subplots_adjust(right=.9)  # <-- Add space so the colorbar doesn't overlap the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_by_abundance(data, column, high, low):\n",
    "    '''Return only rows where genera have one value of abundance in range(low, high)'''\n",
    "    species_to_keep = data[(data[column] <= high) & (data[column] >= low)]['Genus'].unique()\n",
    "    print species_to_keep[0:5]# \n",
    "    return data[data['Genus'].isin(species_to_keep)]\n",
    "    #return data[data['Genus'] in species_to_keep]\n",
    "\n",
    "filter_by_abundance(data=data_reduced_genus, column='abundance', high=1, low=0.5).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a heat map.  Then we aren't confused by colors, and overwhelmed by the # of labels. \n",
    "d_plot = filter_by_abundance(data=data_reduced_genus, column='abundance', high=.8, low=0.1)\n",
    "\n",
    "def facet_heatmap(data, color, **kws):\n",
    "    data = data.pivot(index='Genus', columns='week', values='abundance')\n",
    "    sns.heatmap(data, cmap=\"YlGnBu\", **kws)  # <-- Pass kwargs to heatmap  cmap used to be 'Blue'\n",
    "\n",
    "with sns.plotting_context(font_scale=7):\n",
    "    g = sns.FacetGrid(d_plot[d_plot['oxy']=='Low'], \n",
    "                      col=\"rep\", size=3, aspect=1)\n",
    "\n",
    "cbar_ax = g.fig.add_axes([.92, .3, .02, .4])  # <-- Create a colorbar axes\n",
    "\n",
    "g = g.map_dataframe(facet_heatmap,\n",
    "                    cbar_ax=cbar_ax,\n",
    "                    vmin=0, vmax=max(d_plot.abundance))  # <-- Specify the colorbar axes and limits\n",
    "\n",
    "g.set_titles(col_template=\"{col_name}\", fontweight='bold', fontsize=18)\n",
    "g.fig.subplots_adjust(right=.9)  # <-- Add space so the colorbar doesn't overlap the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_heatmap(data, high, low, oxy, rep): \n",
    "    # get rid of oxygen levels and replicates if specified.\n",
    "    if oxy is not 'all':\n",
    "        print \"keep only {} oxygen samples\".format(oxy)\n",
    "        data = data[data['oxy']==oxy]\n",
    "    if rep is not 'all':\n",
    "        print \"keep only replicate levels:\", rep\n",
    "        data = data[data['rep'].isin(rep)]        \n",
    "    data = filter_by_abundance(data=data, column='abundance', high=high, low=low)\n",
    "    data['facet_replicate'] = 'replicate ' + data['rep'].astype(str)\n",
    "    \n",
    "    # make height of the plot a function of the number of rows (Genera):\n",
    "    num_data_rows = len(data['Genus'].unique())\n",
    "    plot_size = 2 +  num_data_rows/7\n",
    "    plot_aspect = 2\n",
    "    if num_data_rows > 6:\n",
    "        plot_aspect = .85\n",
    "    if num_data_rows > 9:\n",
    "        plot_aspect = .65\n",
    "    if num_data_rows > 9:\n",
    "        plot_aspect = .6\n",
    "    \n",
    "    def facet_heatmap(data, color, **kws):\n",
    "        data = data.pivot(index='Genus', columns='week', values='abundance')\n",
    "        sns.heatmap(data, cmap=\"YlGnBu\", **kws)  # <-- Pass kwargs to heatmap  cmap used to be 'Blue'\n",
    "    \n",
    "    with sns.plotting_context(font_scale=7):\n",
    "        g = sns.FacetGrid(data, col='facet_replicate', size=plot_size, aspect=plot_aspect)\n",
    "    \n",
    "    cbar_ax = g.fig.add_axes([.92, .3, .02, .4])  # <-- Create a colorbar axes\n",
    "    \n",
    "    g = g.map_dataframe(facet_heatmap,\n",
    "                        cbar_ax=cbar_ax,\n",
    "                        vmin=0, vmax=max(data.abundance))  # <-- Specify the colorbar axes and limits\n",
    "    \n",
    "    g.set_titles(col_template=\"{col_name}\", fontweight='bold', fontsize=18)\n",
    "    g.set_axis_labels('week')\n",
    "    g.fig.subplots_adjust(right=.9)  # <-- Add space so the colorbar doesn't overlap the plot\n",
    "    \n",
    "    # add a supertitle. \n",
    "    plt.subplots_adjust(top=0.80)\n",
    "    supertitle = str(low) + ' < abundance < ' + str(high) + ', {} oxygen'.format(oxy) \n",
    "    g.fig.suptitle(supertitle, size=14)\n",
    "    \n",
    "    # write a filename and save.\n",
    "    plot_dir = './plots/'\n",
    "    filename = oxy + \"_oxygen--{0}_to_{1}_abundance\".format(low,high)\n",
    "    print 'filename:', filename\n",
    "    g.savefig(plot_dir + filename + '.pdf')\n",
    "\n",
    "# demo:\n",
    "plot_heatmap(data=data_reduced_genus, high=0.6, low = 0.5, oxy='High', rep='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_heatmap(data=data_reduced_genus, high=1, low = 0.1, oxy='High', rep='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_heatmap(data=data_reduced_genus, high=.5, low = 0.02, oxy='Low', rep='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop through the ranges we want. \n",
    "oxy_conditions = ['Low', 'High']\n",
    "range_pairs = [(0.01, 1), (0.05, 1), (0.1, 1), (0.5, 1)]\n",
    "for o in oxy_conditions:\n",
    "    for r in range_pairs:\n",
    "        #print \"oxygen:\", o\n",
    "        #print \"range:\", r\n",
    "        #print \"range (low):\", r[0]\n",
    "        plot_heatmap(data=data_reduced_genus, low =r[0], high=r[1], oxy=o, rep='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop through the ranges we want. \n",
    "oxy_conditions = ['Low', 'High']\n",
    "max_abundance = 0.6\n",
    "range_pairs = [(0.01, max_abundance), (0.05, max_abundance), (0.1, max_abundance), (0.5, max_abundance)]\n",
    "for o in oxy_conditions:\n",
    "    for r in range_pairs:\n",
    "        #print \"oxygen:\", o\n",
    "        #print \"range:\", r\n",
    "        #print \"range (low):\", r[0]\n",
    "        plot_heatmap(data=data_reduced_genus, low =r[0], high=r[1], oxy=o, rep='all')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# INC: map custom colors on.\n",
    "data_files = ['a', 'b', 'c', 'd']\n",
    "# colors: http://colorbrewer2.org/?type=diverging&scheme=RdYlBu&n=9\n",
    "colorblind_safe_9 = [(215,48,39 ), # red\n",
    "                    (244,109,67 ), # orange\n",
    "                    (253,174,97 ), # light orange\n",
    "                    (254,224,144), # orange-yellow\n",
    "                    (255,255,191), # yellow \n",
    "                    (224,243,248), # very light blue\n",
    "                    (171,217,233), # lighter blue\n",
    "                    (116,173,209), # light blue\n",
    "                    (69,117,180 )] # blue\n",
    "print colorblind_safe_9\n",
    "#colors = dict(zip(data_files, mpl_colors))\n",
    "#print colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
